import{_ as o}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as p,o as c,c as l,d as n,b as s,a as e,e as t}from"./app-NjPjOQVL.js";const i={},r=n("h1",{id:"pytorch-的配置与基本操作",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#pytorch-的配置与基本操作"},[n("span",null,"Pytorch 的配置与基本操作")])],-1),d={href:"https://pytorch.org/",target:"_blank",rel:"noopener noreferrer"},u={href:"https://tangshusen.me/Dive-into-DL-PyTorch/",target:"_blank",rel:"noopener noreferrer"},m={href:"https://zh.d2l.ai/index.html",target:"_blank",rel:"noopener noreferrer"},h=n("h2",{id:"miniconda-配置-pytorch",tabindex:"-1"},[n("a",{class:"header-anchor",href:"#miniconda-配置-pytorch"},[n("span",null,"Miniconda 配置 Pytorch")])],-1),k=n("p",null,"由于 poetry 配置 pytorch 很麻烦，所以我把 pytorch 配置在了 linux 环境下，并且采取 miniconda 作为包管理器。",-1),y={href:"https://pytorch.org/get-started/locally/",target:"_blank",rel:"noopener noreferrer"},g=n("code",null,"nvidia-smi",-1),v=n("code",null,"CUDA",-1),b=t(`<ul><li><code>Pytorch Build</code> Stable(2.2.0)</li><li><code>Your OS</code> linux</li><li><code>Package</code> conda</li><li><code>Lanuage</code> python</li><li><code>Compute Platform</code> CUDA 11.8 运行以下代码来配置环境：</li></ul><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>conda <span class="token function">install</span> pytorch torchvision torchaudio pytorch-cuda<span class="token operator">=</span><span class="token number">11.8</span> <span class="token parameter variable">-c</span> pytorch <span class="token parameter variable">-c</span> nvidia
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><blockquote><p><strong>踩坑：</strong></p><p>如果决定采用<code>conda</code>做包管理器，就老老实实用<code>conda</code>创建虚拟环境，并且在虚拟环境中安装<code>pytorch</code>，GPU 加速版会大概占用 7～8 GB 空间，请注意磁盘空间的规划。不要像我一样没搞清楚，用了<code>conda</code>管理环境，又反用<code>pip</code>作包管理，最后这个环境整了一天才整出来</p></blockquote><ul><li>安装脚本</li></ul><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>conda create <span class="token parameter variable">-n</span> torch <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.9</span>
conda activate torch
conda <span class="token function">install</span> pytorch torchvision torchaudio pytorch-cuda<span class="token operator">=</span><span class="token number">11.8</span> <span class="token parameter variable">-c</span> pytorch <span class="token parameter variable">-c</span> nvidia
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>使用：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="tensor-创建及基本操作" tabindex="-1"><a class="header-anchor" href="#tensor-创建及基本操作"><span>Tensor 创建及基本操作</span></a></h2><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 指定数据创建 tensor</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># 创建 2x3 空 tensor</span>
x1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># 创建 2x3 随机 tensor</span>
x2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>  <span class="token comment"># 创建 long 型的 0 tensor</span>
x3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># 创建 2x3 全 1 tensor</span>
<span class="token builtin">tuple</span> <span class="token operator">=</span> x1<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 返回一个 tuple，任何对于 tuple 的操作都可以适用</span>

<span class="token comment"># 加法</span>
x <span class="token operator">+</span> y
torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> out<span class="token operator">=</span>result<span class="token punctuation">)</span> <span class="token comment"># 通过 out 参数指定输出</span>
y<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># pytorch 的 inplace 操作都有在最后加上下划线</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="索引" tabindex="-1"><a class="header-anchor" href="#索引"><span>索引</span></a></h3><p>可以采用类似 numpy 的索引，<code>y=x[0,:]</code>，但是索引出的数据与原数据共享内存，修改一个另一个也会改变。</p><h3 id="改变形状" tabindex="-1"><a class="header-anchor" href="#改变形状"><span>改变形状</span></a></h3><p><code>view(*size)</code>可以改变<code>tensor</code>的形状，同理，与原数据共享内存，可以理解为：view 仅仅是改变了对这个张量的观察角度，内部数据并未改变。</p><p>如果需要返回一个新的独立副本，应该先<code>clone</code>再<code>view</code>，即：<code>x.clone().view(3,5)</code>\\</p><h3 id="线性代数" tabindex="-1"><a class="header-anchor" href="#线性代数"><span>线性代数</span></a></h3><p>与 MATLAB 语法类似：</p><table><thead><tr><th style="text-align:center;">语法</th><th style="text-align:left;">功能</th></tr></thead><tbody><tr><td style="text-align:center;"><code>trace</code></td><td style="text-align:left;">矩阵的迹</td></tr><tr><td style="text-align:center;"><code>diag</code></td><td style="text-align:left;">对角线元素</td></tr><tr><td style="text-align:center;"><code>triu/tril</code></td><td style="text-align:left;">上三角或下三角矩阵</td></tr><tr><td style="text-align:center;"><code>mm</code></td><td style="text-align:left;">矩阵乘法</td></tr><tr><td style="text-align:center;"><code>t</code></td><td style="text-align:left;">矩阵转置</td></tr><tr><td style="text-align:center;"><code>dot</code></td><td style="text-align:left;">矩阵内积</td></tr><tr><td style="text-align:center;"><code>inverse</code></td><td style="text-align:left;">求逆矩阵</td></tr><tr><td style="text-align:center;"><code>svd</code></td><td style="text-align:left;">奇异值分解</td></tr></tbody></table><h3 id="tensor-转-numpy" tabindex="-1"><a class="header-anchor" href="#tensor-转-numpy"><span>Tensor 转 numpy</span></a></h3><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> a<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="numpy-转-tensor" tabindex="-1"><a class="header-anchor" href="#numpy-转-tensor"><span>numpy 转 Tensor</span></a></h3><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> a<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container important"><p class="hint-container-title">注意</p><p>以上两种方法得到的 Tensor/numpy 共享内存，改变一个另一个也会改变。</p></div><h2 id="自动求梯度" tabindex="-1"><a class="header-anchor" href="#自动求梯度"><span>自动求梯度</span></a></h2><h3 id="function对象" tabindex="-1"><a class="header-anchor" href="#function对象"><span><code>Function</code>对象</span></a></h3><p>如果将其属性<code>.requires_grad</code>设置为<code>True</code>，它将开始追踪 (track) 在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后，可以调用<code>.backward()</code>来完成所有梯度计算。此<code>Tensor</code>的梯度将累积到<code>.grad</code>属性中。</p><p>如果不想要被继续追踪，可以调用<code>.detach()</code>将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用<code>with torch.no_grad()</code>将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数<code>（requires_grad=True）</code>的梯度。</p><p><code>Function</code>是另外一个很重要的类。<code>Tensor</code>和<code>Function</code>互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。每个<code>Tensor</code>都有一个<code>.grad_fn</code>属性，该属性即创建该<code>Tensor</code>的<code>Function</code>, 就是说该<code>Tensor</code>是不是通过某些运算得到的，若是，则<code>grad_fn</code>返回一个与这些运算相关的对象，否则是<code>None</code>。</p><div class="hint-container tip"><p class="hint-container-title">提示</p><p>这个 Function 能够反映该<code>Tensor</code>是如何被创建的，<code>print(x.grad_fn)</code>后可以显示其对象名称，包括但不限于：<code>&lt;AddBackward&gt;</code> , <code>&lt;MeanBackward1&gt;</code>, <code>&lt;SumBackward0&gt;</code></p></div><p>所以，梯度链一定是从一个 <code>Tensor(requires_grad=True)</code> 被创建开始的，这个 <code>Tensor</code>被称作叶子节点，Pytorch 提供了 <code>is_leaf()</code> 函数来角读取其是否为叶子节点。</p><h3 id="梯度" tabindex="-1"><a class="header-anchor" href="#梯度"><span>梯度</span></a></h3>`,30),x={href:"http://colah.github.io/posts/2015-08-Backprop/",target:"_blank",rel:"noopener noreferrer"},_=t(`<div class="hint-container important"><p class="hint-container-title">前向传播与反向传播区别</p><p><strong>前向传播</strong>：只能获得一个输出量对指定自变量的梯度</p><p><strong>反向传播</strong>：遍历一次就可以获得输出量对于计算流图中任意节点的梯度</p></div><p><strong>反向传播</strong>的过程是累加的（这一部分还并没有找到相关原理的文章做支撑，暂且先记住），所以在反向传播之前需要将梯度清零。</p><h4 id="举例说明" tabindex="-1"><a class="header-anchor" href="#举例说明"><span>举例说明</span></a></h4><p>现有如下 python 程序：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">2</span>
z <span class="token operator">=</span> y <span class="token operator">*</span> y <span class="token operator">*</span> <span class="token number">3</span>
out <span class="token operator">=</span> z<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,5),f=n("p",{class:"katex-block"},[n("span",{class:"katex-display"},[n("span",{class:"katex"},[n("span",{class:"katex-mathml"},[n("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[n("semantics",null,[n("mrow",null,[n("mi",null,"o"),n("mi",null,"u"),n("mi",null,"t"),n("mo",null,"="),n("mfrac",null,[n("mn",null,"1"),n("mn",null,"4")]),n("msubsup",null,[n("mi",{mathvariant:"normal"},"Σ"),n("mrow",null,[n("mi",null,"i"),n("mo",null,"="),n("mn",null,"1")]),n("mn",null,"4")]),n("mn",null,"3"),n("mo",{stretchy:"false"},"("),n("msub",null,[n("mi",null,"x"),n("mi",null,"i")]),n("mo",null,"+"),n("mn",null,"2"),n("msup",null,[n("mo",{stretchy:"false"},")"),n("mn",null,"2")])]),n("annotation",{encoding:"application/x-tex"}," out=\\frac{1}{4}\\Sigma_{i=1}^{4}3(x_i+2)^2 ")])])]),n("span",{class:"katex-html","aria-hidden":"true"},[n("span",{class:"base"},[n("span",{class:"strut",style:{height:"0.6151em"}}),n("span",{class:"mord mathnormal"},"o"),n("span",{class:"mord mathnormal"},"u"),n("span",{class:"mord mathnormal"},"t"),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),n("span",{class:"mrel"},"="),n("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"2.0074em","vertical-align":"-0.686em"}}),n("span",{class:"mord"},[n("span",{class:"mopen nulldelimiter"}),n("span",{class:"mfrac"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"1.3214em"}},[n("span",{style:{top:"-2.314em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"mord"},[n("span",{class:"mord"},"4")])]),n("span",{style:{top:"-3.23em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),n("span",{style:{top:"-3.677em"}},[n("span",{class:"pstrut",style:{height:"3em"}}),n("span",{class:"mord"},[n("span",{class:"mord"},"1")])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.686em"}},[n("span")])])])]),n("span",{class:"mclose nulldelimiter"})]),n("span",{class:"mord"},[n("span",{class:"mord"},"Σ"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.8641em"}},[n("span",{style:{top:"-2.453em","margin-left":"0em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mathnormal mtight"},"i"),n("span",{class:"mrel mtight"},"="),n("span",{class:"mord mtight"},"1")])])]),n("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},[n("span",{class:"mord mtight"},"4")])])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.247em"}},[n("span")])])])])]),n("span",{class:"mord"},"3"),n("span",{class:"mopen"},"("),n("span",{class:"mord"},[n("span",{class:"mord mathnormal"},"x"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t vlist-t2"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.3117em"}},[n("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mathnormal mtight"},"i")])])]),n("span",{class:"vlist-s"},"​")]),n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.15em"}},[n("span")])])])])]),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),n("span",{class:"mbin"},"+"),n("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),n("span",{class:"base"},[n("span",{class:"strut",style:{height:"1.1141em","vertical-align":"-0.25em"}}),n("span",{class:"mord"},"2"),n("span",{class:"mclose"},[n("span",{class:"mclose"},")"),n("span",{class:"msupsub"},[n("span",{class:"vlist-t"},[n("span",{class:"vlist-r"},[n("span",{class:"vlist",style:{height:"0.8641em"}},[n("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[n("span",{class:"pstrut",style:{height:"2.7em"}}),n("span",{class:"sizing reset-size6 size3 mtight"},[n("span",{class:"mord mtight"},"2")])])])])])])])])])])])],-1),T=t(`<p>现在对<code>out</code>反向传播，并求<code>out</code>对<code>x</code>的梯度（在反向传播之前需要将 x 的梯度清零）。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>x<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
<span class="token comment"># 输出</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4.5000</span><span class="token punctuation">,</span> <span class="token number">4.5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">4.5000</span><span class="token punctuation">,</span> <span class="token number">4.5000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p>现在看不懂可以不用纠结，因为我也不会（</p></blockquote>`,3);function P(z,w){const a=p("ExternalLinkIcon");return c(),l("div",null,[r,n("p",null,[n("a",d,[s("官网"),e(a)])]),n("p",null,[n("a",u,[s("《动手学深度学习-Pytorch 版》学习文档"),e(a)])]),n("p",null,[n("a",m,[s("《动手学深度学习》原书文档"),e(a)])]),h,k,n("p",null,[s("到"),n("a",y,[s("官网的 Get started 文档"),e(a)]),s(" 选择你的 PC 端配置，可以在终端用"),g,s("命令查看 PC 的 "),v,s(" 版本。我的配置是：")]),b,n("p",null,[s("首先我们得明白在计算流中反向传播的概念，推荐参考"),n("a",x,[s("colah's blog 有关反向传播的理解"),e(a)])]),_,f,T])}const D=o(i,[["render",P],["__file","1pytorch.html.vue"]]),M=JSON.parse('{"path":"/code/python/pytorch/1pytorch.html","title":"Pytorch 的配置与基本操作","lang":"zh-CN","frontmatter":{"date":"2024-02-16T00:00:00.000Z","description":"Pytorch 的配置与基本操作 官网 《动手学深度学习-Pytorch 版》学习文档 《动手学深度学习》原书文档 Miniconda 配置 Pytorch 由于 poetry 配置 pytorch 很麻烦，所以我把 pytorch 配置在了 linux 环境下，并且采取 miniconda 作为包管理器。 到官网的 Get started 文档 选择...","head":[["meta",{"property":"og:url","content":"https://dream-oyh.github.io/code/python/pytorch/1pytorch.html"}],["meta",{"property":"og:site_name","content":"Dream_oyh 的 blog"}],["meta",{"property":"og:title","content":"Pytorch 的配置与基本操作"}],["meta",{"property":"og:description","content":"Pytorch 的配置与基本操作 官网 《动手学深度学习-Pytorch 版》学习文档 《动手学深度学习》原书文档 Miniconda 配置 Pytorch 由于 poetry 配置 pytorch 很麻烦，所以我把 pytorch 配置在了 linux 环境下，并且采取 miniconda 作为包管理器。 到官网的 Get started 文档 选择..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-02-17T14:32:00.000Z"}],["meta",{"property":"article:author","content":"OYH"}],["meta",{"property":"article:published_time","content":"2024-02-16T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-02-17T14:32:00.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Pytorch 的配置与基本操作\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-02-16T00:00:00.000Z\\",\\"dateModified\\":\\"2024-02-17T14:32:00.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"OYH\\",\\"email\\":\\"19859860010@163.com\\"}]}"]]},"headers":[{"level":2,"title":"Miniconda 配置 Pytorch","slug":"miniconda-配置-pytorch","link":"#miniconda-配置-pytorch","children":[]},{"level":2,"title":"Tensor 创建及基本操作","slug":"tensor-创建及基本操作","link":"#tensor-创建及基本操作","children":[{"level":3,"title":"索引","slug":"索引","link":"#索引","children":[]},{"level":3,"title":"改变形状","slug":"改变形状","link":"#改变形状","children":[]},{"level":3,"title":"线性代数","slug":"线性代数","link":"#线性代数","children":[]},{"level":3,"title":"Tensor 转 numpy","slug":"tensor-转-numpy","link":"#tensor-转-numpy","children":[]},{"level":3,"title":"numpy 转 Tensor","slug":"numpy-转-tensor","link":"#numpy-转-tensor","children":[]}]},{"level":2,"title":"自动求梯度","slug":"自动求梯度","link":"#自动求梯度","children":[{"level":3,"title":"Function对象","slug":"function对象","link":"#function对象","children":[]},{"level":3,"title":"梯度","slug":"梯度","link":"#梯度","children":[]}]}],"git":{"createdTime":1708100213000,"updatedTime":1708180320000,"contributors":[{"name":"dream_linux","email":"1399541701@qq.com","commits":1},{"name":"dream同学0","email":"1399541701@qq.com","commits":1}]},"readingTime":{"minutes":4.3,"words":1291},"filePathRelative":"code/python/pytorch/1pytorch.md","localizedDate":"2024年2月16日","excerpt":"\\n<p><a href=\\"https://pytorch.org/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">官网</a></p>\\n<p><a href=\\"https://tangshusen.me/Dive-into-DL-PyTorch/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">《动手学深度学习-Pytorch 版》学习文档</a></p>\\n<p><a href=\\"https://zh.d2l.ai/index.html\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">《动手学深度学习》原书文档</a></p>","autoDesc":true}');export{D as comp,M as data};
