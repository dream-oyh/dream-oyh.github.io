import{_ as l}from"./plugin-vue_export-helper-DlAUqK2U.js";import{r as o,o as c,c as p,d as s,b as n,a as e,e as t}from"./app-BUV4GxTm.js";const r={},i=s("h1",{id:"pytorch-的配置与基本操作",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#pytorch-的配置与基本操作"},[s("span",null,"Pytorch 的配置与基本操作")])],-1),d={href:"https://pytorch.org/",target:"_blank",rel:"noopener noreferrer"},m={href:"https://tangshusen.me/Dive-into-DL-PyTorch/",target:"_blank",rel:"noopener noreferrer"},u={href:"https://zh.d2l.ai/index.html",target:"_blank",rel:"noopener noreferrer"},h=s("h2",{id:"miniconda-配置-pytorch",tabindex:"-1"},[s("a",{class:"header-anchor",href:"#miniconda-配置-pytorch"},[s("span",null,"Miniconda 配置 Pytorch")])],-1),g=s("p",null,"由于 poetry 配置 pytorch 很麻烦，所以我把 pytorch 配置在了 linux 环境下，并且采取 miniconda 作为包管理器。",-1),k={href:"https://pytorch.org/get-started/locally/",target:"_blank",rel:"noopener noreferrer"},y=s("code",null,"nvidia-smi",-1),v=s("code",null,"CUDA",-1),b=t(`<ul><li><code>Pytorch Build</code> Stable(2.2.0)</li><li><code>Your OS</code> linux</li><li><code>Package</code> conda</li><li><code>Lanuage</code> python</li><li><code>Compute Platform</code> CUDA 11.8 运行以下代码来配置环境：</li></ul><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>conda <span class="token function">install</span> pytorch torchvision torchaudio pytorch-cuda<span class="token operator">=</span><span class="token number">11.8</span> <span class="token parameter variable">-c</span> pytorch <span class="token parameter variable">-c</span> nvidia
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><blockquote><p><strong>踩坑：</strong></p><p>如果决定采用<code>conda</code>做包管理器，就老老实实用<code>conda</code>创建虚拟环境，并且在虚拟环境中安装<code>pytorch</code>，GPU 加速版会大概占用 7 ～ 8 GB 空间，请注意磁盘空间的规划。不要像我一样没搞清楚，用了<code>conda</code>管理环境，又反用<code>pip</code>作包管理，最后这个环境整了一天才整出来</p></blockquote><ul><li>安装脚本</li></ul><div class="language-bash line-numbers-mode" data-ext="sh" data-title="sh"><pre class="language-bash"><code>conda create <span class="token parameter variable">-n</span> torch <span class="token assign-left variable">python</span><span class="token operator">=</span><span class="token number">3.9</span>
conda activate torch
conda <span class="token function">install</span> pytorch torchvision torchaudio pytorch-cuda<span class="token operator">=</span><span class="token number">11.8</span> <span class="token parameter variable">-c</span> pytorch <span class="token parameter variable">-c</span> nvidia
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>使用：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><h2 id="tensor-创建及基本操作" tabindex="-1"><a class="header-anchor" href="#tensor-创建及基本操作"><span>Tensor 创建及基本操作</span></a></h2><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment"># 指定数据创建 tensor</span>
y <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># 创建 2x3 空 tensor</span>
x1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># 创建 2x3 随机 tensor</span>
x2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span><span class="token builtin">long</span><span class="token punctuation">)</span>  <span class="token comment"># 创建 long 型的 0 tensor</span>
x3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment"># 创建 2x3 全 1 tensor</span>
<span class="token builtin">tuple</span> <span class="token operator">=</span> x1<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment"># 返回一个 tuple，任何对于 tuple 的操作都可以适用</span>

<span class="token comment"># 加法</span>
x <span class="token operator">+</span> y
torch<span class="token punctuation">.</span>add<span class="token punctuation">(</span>x<span class="token punctuation">,</span> y<span class="token punctuation">,</span> out<span class="token operator">=</span>result<span class="token punctuation">)</span> <span class="token comment"># 通过 out 参数指定输出</span>
y<span class="token punctuation">.</span>add_<span class="token punctuation">(</span>x<span class="token punctuation">)</span>  <span class="token comment"># pytorch 的 inplace 操作都有在最后加上下划线</span>

</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="torch-中的乘法" tabindex="-1"><a class="header-anchor" href="#torch-中的乘法"><span>Torch 中的乘法</span></a></h3>`,10),x=s("code",null,"Torch",-1),_={href:"https://www.cnblogs.com/HOMEofLowell/p/15962140.html",target:"_blank",rel:"noopener noreferrer"},f=s("div",{class:"hint-container tip"},[s("p",{class:"hint-container-title"},"总结"),s("ul",null,[s("li",null,[s("code",null,"torch.mul()"),n(" 有广播机制，各个元素相乘")]),s("li",null,[s("code",null,"torch.multiply()"),n(" 与"),s("code",null,"torch.mul()"),n("相同")]),s("li",null,[s("code",null,"torch.dot()"),n("计算两向量的点积")]),s("li",null,[s("code",null,"torch.mv()"),n(" 计算矩阵和向量的乘积（二者一定有一个维度尺寸相同）")]),s("li",null,[s("code",null,"torch.mm()"),n(" 线代中严格的矩阵乘法")]),s("li",null,[s("code",null,"torch.bmm()"),n(" 批量矩阵相乘，比如"),s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[s("semantics",null,[s("mrow",null,[s("mo",{stretchy:"false"},"["),s("mi",null,"b"),s("mo",null,"×"),s("mi",null,"m"),s("mo",null,"×"),s("mi",null,"n"),s("mo",{stretchy:"false"},"]"),s("mo",null,"∗"),s("mo",{stretchy:"false"},"["),s("mi",null,"b"),s("mo",null,"×"),s("mi",null,"n"),s("mo",null,"×"),s("mi",null,"p"),s("mo",{stretchy:"false"},"]"),s("mo",null,"="),s("mo",{stretchy:"false"},"["),s("mi",null,"b"),s("mo",null,"×"),s("mi",null,"m"),s("mo",null,"×"),s("mi",null,"p"),s("mo",{stretchy:"false"},"]")]),s("annotation",{encoding:"application/x-tex"},"[b\\times m\\times n] * [b\\times n \\times p] = [b\\times m \\times p]")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"["),s("span",{class:"mord mathnormal"},"b"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6667em","vertical-align":"-0.0833em"}}),s("span",{class:"mord mathnormal"},"m"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"n"),s("span",{class:"mclose"},"]"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"∗"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"["),s("span",{class:"mord mathnormal"},"b"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6667em","vertical-align":"-0.0833em"}}),s("span",{class:"mord mathnormal"},"n"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mclose"},"]"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mopen"},"["),s("span",{class:"mord mathnormal"},"b"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6667em","vertical-align":"-0.0833em"}}),s("span",{class:"mord mathnormal"},"m"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"×"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1em","vertical-align":"-0.25em"}}),s("span",{class:"mord mathnormal"},"p"),s("span",{class:"mclose"},"]")])])])]),s("li",null,[s("code",null,"torch.matmul()"),n(" 混合型矩阵乘法，会根据输入维度自动匹配，易出错，不建议使用。")])])],-1),T=s("code",null,"torch.matmul()",-1),w={href:"https://pytorch.org/docs/stable/generated/torch.matmul.html",target:"_blank",rel:"noopener noreferrer"},P=t(`<h3 id="索引" tabindex="-1"><a class="header-anchor" href="#索引"><span>索引</span></a></h3><p>可以采用类似 numpy 的索引，<code>y=x[0,:]</code>，但是索引出的数据与原数据共享内存，修改一个另一个也会改变。</p><h3 id="改变形状" tabindex="-1"><a class="header-anchor" href="#改变形状"><span>改变形状</span></a></h3><p><code>view(*size)</code>可以改变<code>tensor</code>的形状，同理，与原数据共享内存，可以理解为：view 仅仅是改变了对这个张量的观察角度，内部数据并未改变。</p><p>如果需要返回一个新的独立副本，应该先<code>clone</code>再<code>view</code>，即：<code>x.clone().view(3,5)</code>\\</p><h3 id="线性代数" tabindex="-1"><a class="header-anchor" href="#线性代数"><span>线性代数</span></a></h3><p>与 MATLAB 语法类似：</p><table><thead><tr><th style="text-align:center;">语法</th><th style="text-align:left;">功能</th></tr></thead><tbody><tr><td style="text-align:center;"><code>trace</code></td><td style="text-align:left;">矩阵的迹</td></tr><tr><td style="text-align:center;"><code>diag</code></td><td style="text-align:left;">对角线元素</td></tr><tr><td style="text-align:center;"><code>triu/tril</code></td><td style="text-align:left;">上三角或下三角矩阵</td></tr><tr><td style="text-align:center;"><code>mm</code></td><td style="text-align:left;">矩阵乘法</td></tr><tr><td style="text-align:center;"><code>t</code></td><td style="text-align:left;">矩阵转置</td></tr><tr><td style="text-align:center;"><code>dot</code></td><td style="text-align:left;">矩阵内积</td></tr><tr><td style="text-align:center;"><code>inverse</code></td><td style="text-align:left;">求逆矩阵</td></tr><tr><td style="text-align:center;"><code>svd</code></td><td style="text-align:left;">奇异值分解</td></tr></tbody></table><h3 id="tensor-转-numpy" tabindex="-1"><a class="header-anchor" href="#tensor-转-numpy"><span>Tensor 转 numpy</span></a></h3><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> a<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="numpy-转-tensor" tabindex="-1"><a class="header-anchor" href="#numpy-转-tensor"><span>numpy 转 Tensor</span></a></h3><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>
b <span class="token operator">=</span> a<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><div class="hint-container important"><p class="hint-container-title">注意</p><p>以上两种方法得到的 Tensor/numpy 共享内存，改变一个另一个也会改变。</p></div><h3 id="tensor的存储和读取" tabindex="-1"><a class="header-anchor" href="#tensor的存储和读取"><span><code>Tensor</code>的存储和读取</span></a></h3><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token string">&#39;tensor.pt&#39;</span><span class="token punctuation">)</span>  <span class="token comment"># 保存</span>
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&#39;tensor.pt&#39;</span><span class="token punctuation">)</span>  <span class="token comment"># 读取</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p><code>Tensor</code>的保存支持多种数据类型，可以是<code>Tensor</code>，也可以是<code>list</code>和<code>dictionary</code>，保存的 <code>Tensor</code> 和读取的 <code>Tensor</code> 具有相同的类型。<code>Tensor</code>会被保存到以<code>.pt</code>为后缀名的文件中。</p></blockquote><h2 id="自动求梯度" tabindex="-1"><a class="header-anchor" href="#自动求梯度"><span>自动求梯度</span></a></h2><h3 id="function对象" tabindex="-1"><a class="header-anchor" href="#function对象"><span><code>Function</code>对象</span></a></h3><p>如果将其属性<code>.requires_grad</code>设置为<code>True</code>，它将开始追踪 (track) 在其上的所有操作（这样就可以利用链式法则进行梯度传播了）。完成计算后，可以调用<code>.backward()</code>来完成所有梯度计算。此<code>Tensor</code>的梯度将累积到<code>.grad</code>属性中。</p><p>如果不想要被继续追踪，可以调用<code>.detach()</code>将其从追踪记录中分离出来，这样就可以防止将来的计算被追踪，这样梯度就传不过去了。此外，还可以用<code>with torch.no_grad()</code>将不想被追踪的操作代码块包裹起来，这种方法在评估模型的时候很常用，因为在评估模型时，我们并不需要计算可训练参数<code>（requires_grad=True）</code>的梯度。</p><p><code>Function</code>是另外一个很重要的类。<code>Tensor</code>和<code>Function</code>互相结合就可以构建一个记录有整个计算过程的有向无环图（DAG）。每个<code>Tensor</code>都有一个<code>.grad_fn</code>属性，该属性即创建该<code>Tensor</code>的<code>Function</code>, 就是说该<code>Tensor</code>是不是通过某些运算得到的，若是，则<code>grad_fn</code>返回一个与这些运算相关的对象，否则是<code>None</code>。</p><div class="hint-container tip"><p class="hint-container-title">提示</p><p>这个 Function 能够反映该<code>Tensor</code>是如何被创建的，<code>print(x.grad_fn)</code>后可以显示其对象名称，包括但不限于：<code>&lt;AddBackward&gt;</code> , <code>&lt;MeanBackward1&gt;</code>, <code>&lt;SumBackward0&gt;</code></p></div><p>所以，梯度链一定是从一个 <code>Tensor(requires_grad=True)</code> 被创建开始的，这个 <code>Tensor</code>被称作叶子节点，Pytorch 提供了 <code>is_leaf()</code> 函数来角读取其是否为叶子节点。</p><h3 id="梯度" tabindex="-1"><a class="header-anchor" href="#梯度"><span>梯度</span></a></h3>`,24),z={href:"http://colah.github.io/posts/2015-08-Backprop/",target:"_blank",rel:"noopener noreferrer"},M=t(`<div class="hint-container important"><p class="hint-container-title">前向传播与反向传播区别</p><p><strong>前向传播</strong>：只能获得一个输出量对指定自变量的梯度</p><p><strong>反向传播</strong>：遍历一次就可以获得输出量对于计算流图中任意节点的梯度</p></div><p><strong>反向传播</strong>的过程是累加的（这一部分还并没有找到相关原理的文章做支撑，暂且先记住），所以在反向传播之前需要将梯度清零。</p><h4 id="举例说明" tabindex="-1"><a class="header-anchor" href="#举例说明"><span>举例说明</span></a></h4><p>现有如下 python 程序：</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code><span class="token keyword">import</span> torch
x <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
y <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">2</span>
z <span class="token operator">=</span> y <span class="token operator">*</span> y <span class="token operator">*</span> <span class="token number">3</span>
out <span class="token operator">=</span> z<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,5),q=s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mi",null,"o"),s("mi",null,"u"),s("mi",null,"t"),s("mo",null,"="),s("mfrac",null,[s("mn",null,"1"),s("mn",null,"4")]),s("msubsup",null,[s("mi",{mathvariant:"normal"},"Σ"),s("mrow",null,[s("mi",null,"i"),s("mo",null,"="),s("mn",null,"1")]),s("mn",null,"4")]),s("mn",null,"3"),s("mo",{stretchy:"false"},"("),s("msub",null,[s("mi",null,"x"),s("mi",null,"i")]),s("mo",null,"+"),s("mn",null,"2"),s("msup",null,[s("mo",{stretchy:"false"},")"),s("mn",null,"2")])]),s("annotation",{encoding:"application/x-tex"}," out=\\frac{1}{4}\\Sigma_{i=1}^{4}3(x_i+2)^2 ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"0.6151em"}}),s("span",{class:"mord mathnormal"},"o"),s("span",{class:"mord mathnormal"},"u"),s("span",{class:"mord mathnormal"},"t"),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),s("span",{class:"mrel"},"="),s("span",{class:"mspace",style:{"margin-right":"0.2778em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"2.0074em","vertical-align":"-0.686em"}}),s("span",{class:"mord"},[s("span",{class:"mopen nulldelimiter"}),s("span",{class:"mfrac"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.3214em"}},[s("span",{style:{top:"-2.314em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"4")])]),s("span",{style:{top:"-3.23em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"frac-line",style:{"border-bottom-width":"0.04em"}})]),s("span",{style:{top:"-3.677em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"1")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.686em"}},[s("span")])])])]),s("span",{class:"mclose nulldelimiter"})]),s("span",{class:"mord"},[s("span",{class:"mord"},"Σ"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8641em"}},[s("span",{style:{top:"-2.453em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mathnormal mtight"},"i"),s("span",{class:"mrel mtight"},"="),s("span",{class:"mord mtight"},"1")])])]),s("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},[s("span",{class:"mord mtight"},"4")])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.247em"}},[s("span")])])])])]),s("span",{class:"mord"},"3"),s("span",{class:"mopen"},"("),s("span",{class:"mord"},[s("span",{class:"mord mathnormal"},"x"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.3117em"}},[s("span",{style:{top:"-2.55em","margin-left":"0em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mathnormal mtight"},"i")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.15em"}},[s("span")])])])])]),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}}),s("span",{class:"mbin"},"+"),s("span",{class:"mspace",style:{"margin-right":"0.2222em"}})]),s("span",{class:"base"},[s("span",{class:"strut",style:{height:"1.1141em","vertical-align":"-0.25em"}}),s("span",{class:"mord"},"2"),s("span",{class:"mclose"},[s("span",{class:"mclose"},")"),s("span",{class:"msupsub"},[s("span",{class:"vlist-t"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"0.8641em"}},[s("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[s("span",{class:"pstrut",style:{height:"2.7em"}}),s("span",{class:"sizing reset-size6 size3 mtight"},[s("span",{class:"mord mtight"},"2")])])])])])])])])])])])],-1),B=t(`<p>现在对<code>out</code>反向传播，并求<code>out</code>对<code>x</code>的梯度（在反向传播之前需要将 x 的梯度清零）。</p><div class="language-python line-numbers-mode" data-ext="py" data-title="py"><pre class="language-python"><code>x<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
out<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span>
<span class="token comment"># 输出</span>
tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4.5000</span><span class="token punctuation">,</span> <span class="token number">4.5000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
        <span class="token punctuation">[</span><span class="token number">4.5000</span><span class="token punctuation">,</span> <span class="token number">4.5000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><blockquote><p>现在看不懂可以不用纠结，因为我也不会（</p></blockquote><h2 id="学习资料" tabindex="-1"><a class="header-anchor" href="#学习资料"><span>学习资料</span></a></h2>`,4),D={href:"https://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html",target:"_blank",rel:"noopener noreferrer"};function L(C,N){const a=o("ExternalLinkIcon");return c(),p("div",null,[i,s("p",null,[s("a",d,[n("官网"),e(a)])]),s("p",null,[s("a",m,[n("《动手学深度学习-Pytorch 版》学习文档"),e(a)])]),s("p",null,[s("a",u,[n("《动手学深度学习》原书文档"),e(a)])]),h,g,s("p",null,[n("到"),s("a",k,[n("官网的 Get started 文档"),e(a)]),n(" 选择你的 PC 端配置，可以在终端用"),y,n("命令查看 PC 的 "),v,n(" 版本。我的配置是：")]),b,s("p",null,[x,n("包中包括多种乘法方式，具体的细则可以看"),s("a",_,[n("这篇博客"),e(a)])]),f,s("p",null,[n("的那是该博文中没有明确写"),T,n("的用法，具体可以查"),s("a",w,[n("官方文档"),e(a)])]),P,s("p",null,[n("首先我们得明白在计算流中反向传播的概念，推荐参考"),s("a",z,[n("colah's blog 有关反向传播的理解"),e(a)])]),M,q,B,s("ul",null,[s("li",null,[s("a",D,[n("台大李宏毅深度学习作业安排"),e(a)])])])])}const G=l(r,[["render",L],["__file","1pytorch.html.vue"]]),S=JSON.parse('{"path":"/code/python/pytorch/1pytorch.html","title":"Pytorch 的配置与基本操作","lang":"zh-CN","frontmatter":{"date":"2024-02-16T00:00:00.000Z","description":"Pytorch 的配置与基本操作 官网 《动手学深度学习-Pytorch 版》学习文档 《动手学深度学习》原书文档 Miniconda 配置 Pytorch 由于 poetry 配置 pytorch 很麻烦，所以我把 pytorch 配置在了 linux 环境下，并且采取 miniconda 作为包管理器。 到官网的 Get started 文档 选择...","head":[["meta",{"property":"og:url","content":"https://dream-oyh.github.io/code/python/pytorch/1pytorch.html"}],["meta",{"property":"og:site_name","content":"Dream_oyh 的 blog"}],["meta",{"property":"og:title","content":"Pytorch 的配置与基本操作"}],["meta",{"property":"og:description","content":"Pytorch 的配置与基本操作 官网 《动手学深度学习-Pytorch 版》学习文档 《动手学深度学习》原书文档 Miniconda 配置 Pytorch 由于 poetry 配置 pytorch 很麻烦，所以我把 pytorch 配置在了 linux 环境下，并且采取 miniconda 作为包管理器。 到官网的 Get started 文档 选择..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-02-19T14:31:50.000Z"}],["meta",{"property":"article:author","content":"OYH"}],["meta",{"property":"article:published_time","content":"2024-02-16T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-02-19T14:31:50.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Pytorch 的配置与基本操作\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2024-02-16T00:00:00.000Z\\",\\"dateModified\\":\\"2024-02-19T14:31:50.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"OYH\\",\\"email\\":\\"19859860010@163.com\\"}]}"]]},"headers":[{"level":2,"title":"Miniconda 配置 Pytorch","slug":"miniconda-配置-pytorch","link":"#miniconda-配置-pytorch","children":[]},{"level":2,"title":"Tensor 创建及基本操作","slug":"tensor-创建及基本操作","link":"#tensor-创建及基本操作","children":[{"level":3,"title":"Torch 中的乘法","slug":"torch-中的乘法","link":"#torch-中的乘法","children":[]},{"level":3,"title":"索引","slug":"索引","link":"#索引","children":[]},{"level":3,"title":"改变形状","slug":"改变形状","link":"#改变形状","children":[]},{"level":3,"title":"线性代数","slug":"线性代数","link":"#线性代数","children":[]},{"level":3,"title":"Tensor 转 numpy","slug":"tensor-转-numpy","link":"#tensor-转-numpy","children":[]},{"level":3,"title":"numpy 转 Tensor","slug":"numpy-转-tensor","link":"#numpy-转-tensor","children":[]},{"level":3,"title":"Tensor的存储和读取","slug":"tensor的存储和读取","link":"#tensor的存储和读取","children":[]}]},{"level":2,"title":"自动求梯度","slug":"自动求梯度","link":"#自动求梯度","children":[{"level":3,"title":"Function对象","slug":"function对象","link":"#function对象","children":[]},{"level":3,"title":"梯度","slug":"梯度","link":"#梯度","children":[]}]},{"level":2,"title":"学习资料","slug":"学习资料","link":"#学习资料","children":[]}],"git":{"createdTime":1708100213000,"updatedTime":1708353110000,"contributors":[{"name":"dream_linux","email":"1399541701@qq.com","commits":3},{"name":"dream同学0","email":"1399541701@qq.com","commits":1}]},"readingTime":{"minutes":5.19,"words":1556},"filePathRelative":"code/python/pytorch/1pytorch.md","localizedDate":"2024年2月16日","excerpt":"\\n<p><a href=\\"https://pytorch.org/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">官网</a></p>\\n<p><a href=\\"https://tangshusen.me/Dive-into-DL-PyTorch/\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">《动手学深度学习-Pytorch 版》学习文档</a></p>\\n<p><a href=\\"https://zh.d2l.ai/index.html\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">《动手学深度学习》原书文档</a></p>","autoDesc":true}');export{G as comp,S as data};
