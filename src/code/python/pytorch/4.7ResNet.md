---
date: 2024-02-26
---

# ResNet

> What is ResNet?

简单来说，在模型训练过程中，直接去拟合我们要的$f(x)$并不容易，但是通过测试发现，直接去拟合$f(x)-x$会容易很多，所以残差网络其实就是在一系列卷积计算过后，先减去一个输入矩阵，再通过激活函数计算输出，这样就相当于拟合了$f(x)-x$。

残差网络的结构可以见[原文档](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.11_resnet?id=_511-%e6%ae%8b%e5%b7%ae%e7%bd%91%e7%bb%9c%ef%bc%88resnet%ef%bc%89)，本节主要专注于程序上的实现难点。

## Residual 块

ResNet 沿用了 VGG 中 3×3 卷积层的设计。残差块里首先有 2 个有相同输出通道数的 3×3 卷积层。每个卷积层后接一个批量归一化层和 ReLU 激活函数。然后我们将输入跳过这两个卷积运算后直接加在最后的 ReLU 激活函数前。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的 1×1 卷积层来将输入变换成需要的形状后再做相加运算。

残差块的实现如下。它可以设定输出通道数以及卷积层的步幅。

> 原文档说残差块要给一个超参数，用来判断是否需要引入额外的 1x1 卷积层，但是实际上不需要，只需要通过判断输入和输出的通道数是否相同来决定是否需要引入额外的 1x1 卷积层。

```python
class Residual(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1) -> None:
        super(Residual, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3)
        if in_channels != out_channels:
            self.conv3 = nn.Conv2d(
                in_channels, out_channels, kernel_size=1, stride=stride
            )
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.bn2 = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        y = F.relu(self.bn1(self.conv1(x)))
        y = self.bn2(self.conv2(y))
        if self.conv3:
            x = self.conv3(x)
        return F.relu(y + x)
```

## 残差网络

ResNet 的前两层跟之前介绍的 GoogLeNet 中的一样：在输出通道数为 64、步幅为 2 的 7×7 卷积层后接步幅为 2 的 3×3 的最大池化层。不同之处在于 ResNet 每个卷积层后增加的批量归一化层。

