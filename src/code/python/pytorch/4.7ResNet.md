# ResNet

> What is ResNet?

简单来说，在模型训练过程中，直接去拟合我们要的$f(x)$并不容易，但是通过测试发现，直接去拟合$f(x)-x$会容易很多，所以残差网络其实就是在一系列卷积计算过后，先减去一个输入矩阵，再通过激活函数计算输出，这样就相当于拟合了$f(x)-x$。

残差网络的结构可以见[原文档](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter05_CNN/5.11_resnet?id=_511-%e6%ae%8b%e5%b7%ae%e7%bd%91%e7%bb%9c%ef%bc%88resnet%ef%bc%89)，本节主要专注于程序上的实现难点。

## Residual 块

ResNet 沿用了 VGG 中 3×3 卷积层的设计。残差块里首先有 2 个有相同输出通道数的 3×3 卷积层。每个卷积层后接一个批量归一化层和 ReLU 激活函数。然后我们将输入跳过这两个卷积运算后直接加在最后的 ReLU 激活函数前。这样的设计要求两个卷积层的输出与输入形状一样，从而可以相加。如果想改变通道数，就需要引入一个额外的 1×1 卷积层来将输入变换成需要的形状后再做相加运算。

残差块的实现如下。它可以设定输出通道数、是否使用额外的 1×1 卷积层来修改通道数以及卷积层的步幅。

