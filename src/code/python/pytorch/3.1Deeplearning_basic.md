# 深度学习基础

## 模型构建

根据之前章节，我们已经能够发现，利用`nn.Module`和`nn.Sequential`可以很方便地构建模型，并且`nn.Sequential`本身也是`nn.Module`的一个继承类。所以，通过`nn.Module`来构建继承类也是非常通用的模型构建方式，一般来说，在继承类中我们会重载`nn.Module`的`forward()`函数，比如说我们可以通过继承类很容易地构建一个多层感知机模型：

> forward() 函数的目的在于，定义模型的前向计算，如何根据输入的数据 `x`计算出所需要的模型输出 `y`。

```python
import torch.nn as nn
class MLP(nn.Module):
  def __init__(self,**kwargs):
    super(MLP, self).__init__(**kwargs)
    self.hidden = nn.Linear(784, 256)  # 隐藏层
    self.act = nn.ReLU()
    self.output = nn.Linear(256, 10)  # 输出层
  def forward(self, x):
    a = self.act(self.hidden(x))
    renturn self.output(a)
```

而`pytorch`库会自动生成`backward`反向传播函数

### `nn.Module`的子类

`Pytorch`提供了丰富的`nn.Module`子类，我们可以直接使用这些子类来构建模型，比如`nn.Sequential`，`nn.Moudlelist`，`nn.ModuleDict`等，这些子类都继承自`nn.Module`。

#### `nn.Sequential`类

`nn.Sequential`中，各层模型可以以多种方式输入，这一点在[线性回归实现的笔记](./2.1linear_regression.md#定义模型)中有提到，总的来说，模型参数支持
直接输入各层、输入有序字典`OrderedDict`或通过`add_module()`函数添加。

#### `nn.ModuleList`类

该类接受一个模块列表作为输入，如：

```python
net = nn.Modulelist([nn.Linear(256,10),nn.ReLU()])
net.append([nn.Linear(10,2)])
```

该类的索引和列表类似，并且支持类似列表的`.append()`和`.extend()`操作。

::: tip 注意
不同于`nn.Sequential`，`nn.ModuleList`类并不支持`forward()`函数，而是需要自己定义。但是区分于传统`列表`，`nn.ModuleList`又能够生成参数，可以通过`nn.MoudleList.parameters()`来访问参数，所以该类只能认为是一个模块的容器，而并不是一个新的网络。
:::

#### `nn.ModuleDict`类

该类接受一个模块字典作为输入，也可以像字典一样操作。

和`nn.ModuleList`与`list`的关系一样，`nn.ModuleDict`没有定义`forward()`函数，需要自己定义，并且他较传统字典，他包括了模型所需要的权重参数。

- 由于三者均为`nn..Module`的子类，所以可以互相嵌套调用。

::: important 不要局限了 Module 的用法

我们在[softmax](./2.3softmax.md#定义和初始化模型)中定义了一个`FlattenLayer`层，这个层不同于一般印象里的线性计算，他只是一个`reshape`的操作，但是我们把这也叫做一个层。也因为如此，我们对于`Module`的认识应该更广泛一些，`Module`提供的`forward`函数可以看作是这个层的`Action`，一种广义上的行为，而不只是前向传播。

对于[丢弃法 (Dropout)](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter03_DL-basics/3.13_dropout?id=_3131-方法)，`nn.Module`也提供了`nn.Dropout(drop_prob)`层，以方便使用。我们需要在全连接层之后加入`Dropout`层，它将在训练模型中以指定的丢弃概率随机丢弃一部分神经元，而在测试模型中不发挥作用。

:::

## 访问模型参数

对于用`Sequential`类构建的网络，我们可以简单地用`net[i]`调用指定层（i 的索引从 0 开始），用`parameters()`和`name_parameters()`调用参数，并且返回一个参数迭代器。

如：

```python
net = nn.Sequential(nn.Linear(256,10),nn.ReLU(),nn.Linear(10,2))
for params in net.parameters()：
    print(params.size())
# 输出结果为：
torch.Size([10, 256])
torch.Size([10])
torch.Size([2, 10])
torch.Size([2])

for name,params in net.named_parameters()：
    print(name,params.size())
# name_parameters() 输出带有名字的参数列表，输出结果为：
0.weight torch.Size([10, 256])
0.bias torch.Size([10])
2.weight torch.Size([2, 10])
2.bias torch.Size([2])
```

::: tip params 的对象类型
`params`本质上也是一个`Tesnor`张量，其类型为`torch.nn.parameter.Parameter`，可以与`Tensor`有同样的操作，如`.data`读取数据，`.grad`求取梯度。
:::

## 初始化模型参数

一般来说，我们需要对权重赋予正态分布的初始化，对偏差清零，赋予常数的初始化，我们可以通过参数的名字对参数进行区分：

```python
for name, param in net.named_parameters():
  if "weight" in name:
    init.normal_(param,mean=0,std=0.01)
  if "bias" in name:
    init.constant_(param,val = 0)
```

### 自定义初始化方法

`pytorch`中对一个张量初始化是不记录梯度的，所以我们需要在`with tensor.no_grad()`环境里配置，比如说初始化使得整体分布于（0,0.01）的标准正态分布，但是权重有一半的概率初始化为 0，另一半参数大于 0。可以这么定义初始化函数：

```python{2,4}
def init_weight_(tensor, mean=0, std=0.01):
  with tensor.no_grad():
    tensor.normal_(mean,std)
    tensor *= (tensor>0).float()

for name, param in net.named_parameters():
  if "weight" in name:
    init_weight_(param,mean=0,std=0.01)
```

### 共享模型参数

只要保证两个层来自同一个数据寄存器（或者说来自同一个变量），即可保证参数共享。如：

```python
linear = nn.Linear(10,10)
net = nn.Sequential(linear,linear)
print(id(net[0])==id(net[1]))
print(id(net[0].weight)==id(net[1].weight))
# 输出
True
True
```

::: tip 值得注意的是
在反向传播过程中，这两层的参数梯度是累加的。
:::

## 自定义层

如前所说，`Module`这玩意是很宽泛的，可以继承它然后来构建很多自定义层。自定义层又分为两种：

- 不带模型参数的自定义层
- 带模型参数的自定义层

> 不带模型参数的自定义层好理解，类似`FlattenLayer`就是一个不带模型参数的自定义层

**本文着重讨论带模型参数的自定义层。**

由前文可知，模型参数应该是`Parameter`类型的，也是`Tensor`的子类，所以我们在继承类的`__init__()`函数中，应该通过`nn.Parameter()`来定义模型参数（定义方式与`Tensor`一致），指定的参数名字也会被`name_parameters()`读取。也可以通过`ParameterList()`或`ParameterDict()`类型来定义（输入一个`Parameter`列表或字典），举例如下：

```python
class MyDense(nn.Module):
    def __init__(self):
        super(MyDense, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])
        self.params.append(nn.Parameter(torch.randn(4, 1)))

    def forward(self, x):
        for i in range(len(self.params)):
            x = torch.mm(x, self.params[i])
        return x
net = MyDense()
print(net)
```

## 保存和加载模型

[查看文档链接](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter04_DL_computation/4.5_read-write)

## GPU 计算

[查看文档链接](https://tangshusen.me/Dive-into-DL-PyTorch/#/chapter04_DL_computation/4.6_use-gpu)
